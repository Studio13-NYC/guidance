{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring ChatGPT and open-source models on slightly harder tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "**Note: This notebook is the code and examples from a related [the blog post](https://medium.com/@marcotcr/exploring-chatgpt-vs-open-source-models-on-slightly-harder-tasks-aa0395c31610) written jointly by Marco Tulio Ribeiro and Scott Lundberg.**  \n",
    "\n",
    "Open-source LLMs like [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) and [MPT](https://www.mosaicml.com/blog/mpt-7b#building-with-mosaicml-platform) are popping up all over the place.  \n",
    "There's been [discussion](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) about how these models compare to commercial LLMs like ChatGPT or Bard, but most of the comparison has been around answers to simple questions.  \n",
    "\n",
    "As an example, the folks at [LMSYSOrg](https://lmsys.org/) did [an interesting analysis](https://lmsys.org/blog/2023-03-30-vicuna/) (+1 for being automated and reproducible) comparing Vicuna-13B to ChatGPT on various short questions, which is great as a comparison of the models as simple chatbots. However, many interesting ways of using these LLMs typically require complex instructions and/or multi-turn conversations, and some prompt engineering.\n",
    "In the 'real world', most people will want to compare different LLM offerings on _their_ problem, with a variety of different prompts.  \n",
    "\n",
    "This notebook is an example of what such an exploration might look like, comparing two open source models (Vicuna-13B, MPT-7b-Chat) with ChatGPT on tasks of varying complexity.\n",
    "\n",
    "The notebook is included here because it is also an example of how Guidance can be used to compare various models easily. We have shortened the discussion and commentary that are in the blog post, but the code and examples are here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup: Solving equations\n",
    "By way of warmup, let's start with a toy task (solving simple polynomial equations), where we can check the output for correctness and shouldn't need much prompt engineering.  \n",
    "This will be similar to the Math category in [here](https://lmsys.org/blog/2023-03-30-vicuna/), with the difference that we will evaluate models as correct / incorrect on ground truth, rather than using GPT-4 to rate the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple function that generates a polynomial with distinct integer roots.\n",
    "This will give us both the input and the ground truth output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roots [9, -5]\n",
      "x^2 - 4.0x - 45.0 = 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def random_polynomial(n_roots, low=1, high=100):\n",
    "    roots = [np.random.randint(low, high) for _ in range(n_roots)]\n",
    "    # unique roots only\n",
    "    while len(set(roots)) != n_roots:\n",
    "        roots = [np.random.randint(low, high) for _ in range(n_roots)]\n",
    "    poly = np.polynomial.polynomial.Polynomial.fromroots(roots)\n",
    "    a = poly.coef.copy()\n",
    "    a = a[::-1]\n",
    "    text= ''\n",
    "    for i, coef in enumerate(a):\n",
    "        if coef == 0:\n",
    "            continue\n",
    "        sign = ' + ' if coef > 0 else ' - '\n",
    "        if i ==0:\n",
    "            sign = ''\n",
    "        elif coef < 0:\n",
    "            coef = -coef\n",
    "        if i == len(a) - 1:\n",
    "            text += f'{sign}{coef}'\n",
    "        else:\n",
    "            if coef == 1:\n",
    "                coef = ''\n",
    "            power = f'^{len(a) - i - 1}' if len(a) - i - 1 > 1 else ''\n",
    "            text += f'{sign}{coef}x{power}'\n",
    "    text += ' = 0'\n",
    "    return roots, text\n",
    "roots, equation = random_polynomial(2, low=-10, high=10)\n",
    "print('Roots', roots)\n",
    "print(equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the models (we use [`guidance`](https://github.com/microsoft/guidance) throughout for easy comparison and control):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "import guidance\n",
    "import transformers\n",
    "path = '/home/marcotcr/.cache/huggingface/hub/vicuna-13b'\n",
    "mpt = guidance.llms.transformers.MPTChat('mosaicml/mpt-7b-chat', device=1)\n",
    "vicuna = guidance.llms.transformers.Vicuna(path, device_map='auto')\n",
    "chatgpt = guidance.llms.OpenAI(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick digression on different syntaxes**: each of these models have a their own _chat syntax_, e.g. here is how the same conversation would look like in Vicuna and MPT (where `[generated response]` is where the model would put its output):  \n",
    "\n",
    "Vicuna:  \n",
    "```\n",
    "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.  \n",
    "USER: Can you please solve the following equation? x^2 + 2x + 1 = 0  \n",
    "ASSISTANT: [generated response] </s>\n",
    "```\n",
    "\n",
    "MPT:  \n",
    "```\n",
    "<|im_start|>system\n",
    "- You are a helpful assistant chatbot trained by MosaicML.  \n",
    "- You answer questions.\n",
    "- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.\"\"\"\n",
    "<|im_end|>\n",
    "<|im_start|>user Can you please solve the following equation? x^2 + 2x + 1 = 0<|im_end|>\n",
    "<|im_start|>assistant [generated response]<|im_end>\n",
    "```\n",
    "\n",
    "To avoid the tediousness translating between these, `guidance` supports a unified chat syntax that gets translated to the model-specific syntax when calling the model.  \n",
    "Here is the prompt we'll use for all models (note how we use ``{{system}}``, ``{{user}}`` and ``{{assistant}}`` tags rather than model-specific separators):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "find_roots = guidance('''\n",
    "{{~#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Please find the roots of the following equation: {{equation}}\n",
    "Think step by step, find the roots, and then say:\n",
    "ROOTS = [root1, root2...]\n",
    "For example, if the roots are 1.3 and 2.2, say ROOTS = [1.3, 2.2].\n",
    "Make sure to use real numbers, not fractions.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the prompt on a very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roots, equation = random_polynomial(2, low=-3, high=3)\n",
    "print(roots)\n",
    "print(equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_gpt = find_roots(llm=chatgpt, equation=equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_vicuna = find_roots(llm=vicuna, equation=equation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_mpt = find_roots(llm=mpt, equation=equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT got it right, while Vicuna and MPT got it wrong (Vicuna didn't even follow the specified format).\n",
    "Let's write a simple regex to parse the output, so we can evaluate this on a few more expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_roots(text):\n",
    "    roots = re.search(r'ROOTS = \\[(.*)\\]', text)\n",
    "    if not roots:\n",
    "        return []\n",
    "    roots = roots.group(1).split(',')\n",
    "    try:\n",
    "        roots = [float(r) for r in roots]\n",
    "    except:\n",
    "        roots = []\n",
    "    return roots\n",
    "def matches_groundtruth(roots, groundtruth):\n",
    "    if len(roots) != len(groundtruth):\n",
    "        return False\n",
    "    gt = np.array(sorted(groundtruth))\n",
    "    roots = np.array(sorted(roots))\n",
    "    return np.all(np.abs(gt - roots) < 1e-3)\n",
    "\n",
    "print('ChatGPT: ', matches_groundtruth(parse_roots(answer_gpt['answer']), roots))\n",
    "print('Vicuna: ', matches_groundtruth(parse_roots(answer_vicuna['answer']), roots))\n",
    "print('MPT: ', matches_groundtruth(parse_roots(answer_mpt['answer']), roots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the prompt on quadratic equations with roots between -20 and 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {\n",
    "    'ChatGPT': [],\n",
    "    'Vicuna': [],\n",
    "    'MPT': []\n",
    "}\n",
    "correct = {\n",
    "    'ChatGPT': [],\n",
    "    'Vicuna': [],\n",
    "    'MPT': []\n",
    "}\n",
    "for _ in range(20):\n",
    "    roots, equation = random_polynomial(2, low=-20, high=20)\n",
    "    answer_gpt = find_roots(llm=chatgpt, equation=equation, silent=True)\n",
    "    answer_vicuna = find_roots(llm=vicuna, equation=equation, silent=True)\n",
    "    answer_mpt = find_roots(llm=mpt, equation=equation, silent=True)\n",
    "    answers['ChatGPT'].append(answer_gpt)\n",
    "    answers['Vicuna'].append(answer_vicuna)\n",
    "    answers['MPT'].append(answer_mpt)\n",
    "    correct['ChatGPT'].append(matches_groundtruth(parse_roots(answer_gpt['answer']), roots))\n",
    "    correct['Vicuna'].append(matches_groundtruth(parse_roots(answer_vicuna['answer']), roots))\n",
    "    correct['MPT'].append(matches_groundtruth(parse_roots(answer_mpt['answer']), roots))\n",
    "\n",
    "print('Frequency of correct answers:')\n",
    "print('ChatGPT: ', np.mean(correct['ChatGPT']))\n",
    "print('Vicuna: ', np.mean(correct['Vicuna']))\n",
    "print('MPT: ', np.mean(correct['MPT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT gets the right roots 80% of the time, while Vicuna and MPT never get them right. Let's see a few errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Vicuna'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['MPT'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna does the math wrong, while MPT does not even attempt to solve it step by step.  \n",
    "ChatGPT also makes some mistakes, often involving some math substep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.where(np.array(correct['ChatGPT']) == False)[0][0]\n",
    "answers['ChatGPT'][error]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Vicuna and MPT failed on quadratic equations, let's look at even simpler equations, such as `x - 10 = 0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_solution = guidance('''\n",
    "{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Please find the solution to the following equation: {{equation}}\n",
    "Think step by step, find the solution, and then say:\n",
    "SOLUTION = [value]\n",
    "For example, if the solution is x=3, say SOLUTION = [3].\n",
    "Make sure to use real numbers, not fractions.\n",
    "{{/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0 max_tokens=300}}\n",
    "{{~/assistant~}}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_solution(text):\n",
    "    solution = re.search(r'SOLUTION = \\[(.*)\\]', text)\n",
    "    if not solution:\n",
    "        return []\n",
    "    try:\n",
    "        return [float(solution.group(1))]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct1d = {\n",
    "    'ChatGPT': [],\n",
    "    'Vicuna': [],\n",
    "    'MPT': []\n",
    "}\n",
    "answers1d = {\n",
    "    'ChatGPT': [],\n",
    "    'Vicuna': [],\n",
    "    'MPT': []\n",
    "}\n",
    "for i in range(20):\n",
    "    print(i)\n",
    "    roots, equation = random_polynomial(1, low=-20, high=20)\n",
    "    answer_gpt = find_solution(llm=chatgpt, equation=equation, silent=False)\n",
    "    answer_vicuna = find_solution(llm=vicuna, equation=equation, silent=False)\n",
    "    answer_mpt = find_solution(llm=mpt, equation=equation, silent=False)\n",
    "    answers1d['ChatGPT'].append(answer_gpt)\n",
    "    answers1d['Vicuna'].append(answer_vicuna)\n",
    "    answers1d['MPT'].append(answer_mpt)\n",
    "    correct1d['ChatGPT'].append(matches_groundtruth(parse_solution(answer_gpt['answer']), roots))\n",
    "    correct1d['Vicuna'].append(matches_groundtruth(parse_solution(answer_vicuna['answer']), roots))\n",
    "    correct1d['MPT'].append(matches_groundtruth(parse_solution(answer_mpt['answer']), roots))\n",
    "\n",
    "print('Frequency of correct answers:')\n",
    "print('ChatGPT: ', np.mean(correct1d['ChatGPT']))\n",
    "print('Vicuna: ', np.mean(correct1d['Vicuna']))\n",
    "print('MPT: ', np.mean(correct1d['MPT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, MPT still fails to solve these. Vicuna still makes some mistakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.where(np.array(correct1d['Vicuna']) == False)[0][0]\n",
    "answers1d['Vicuna'][error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.where(np.array(correct1d['MPT']) == False)[0][0]\n",
    "answers1d['MPT'][error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.where(np.array(correct1d['MPT']) == False)[0][1]\n",
    "answers1d['MPT'][error]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**  \n",
    "This was a very toy task, but served as an example of how to compare models with different chat syntax using the same prompt.  \n",
    "For this particular combination of toy task and prompt, ChatGPT far surpasses Vicuna and MPT in terms of accuracy (measured exactly, because we have ground truth).  \n",
    "Let's now turn to more realistic tasks, where evaluating accuracy is not as straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting snippets + answering questions about meetings\n",
    "Let's say we want our LLM to answer questions (with the relevant conversation segments for grounding) about meeting transcripts.  \n",
    "This is an application where some users might prefer to use open-source LLMs rather than commercial ones, for privacy reasons (maybe I don't want to send all of my meeting data to OpenAI).  \n",
    "Here is a toy meeting transcript to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_transcript = '''John: Alright, so we're all here to discuss the offer we received from Microsoft to buy our startup. What are your thoughts on this?\n",
    "Lucy: Well, I think it's a great opportunity for us. Microsoft is a huge company with a lot of resources, and they could really help us take our product to the next level.\n",
    "Steven: I agree with Lucy. Microsoft has a lot of experience in the tech industry, and they could provide us with the support we need to grow our business.\n",
    "John: I see your point, but I'm a little hesitant about selling our startup. We've put a lot of time and effort into building this company, and I'm not sure if I'm ready to let it go just yet.\n",
    "Lucy: I understand where you're coming from, John, but we have to think about the future of our company. If we sell to Microsoft, we'll have access to their resources and expertise, which could help us grow our business even more.\n",
    "Steven: Right, and let's not forget about the financial benefits. Microsoft is offering us a lot of money for our startup, which could help us invest in new projects and expand our team.\n",
    "John: I see your point, but I still have some reservations. What if Microsoft changes our product or our company culture? What if we lose control over our own business?\n",
    "Steven: You know what, I hadn't thought about this before, but maybe John is right. It would be a shame if our culture changed.\n",
    "Lucy: Those are valid concerns, but we can negotiate the terms of the deal to ensure that we retain some control over our company. And as for the product and culture, we can work with Microsoft to make sure that our vision is still intact.\n",
    "John: But won't we change just by virtue of being absorbed into a big company? I mean, we're a small startup with a very specific culture. Microsoft is a huge corporation with a very different culture. I'm not sure if the two can coexist.\n",
    "Steven: But John, didn't we always plan on being acquired? Won't this be a problem whenever?\n",
    "Lucy: Right\n",
    "John: I just don't want to lose what we've built here.\n",
    "Steven: I share this concern too''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by just trying to get ChatGPT to solve the task for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = 'How does Steven feel about selling?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_attempt1 = guidance('''{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Please answer the following question:\n",
    "Question: {{query}}\n",
    "Extract from the transcript the most relevant segments for the answer, and then answer the question.\n",
    "{{/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}''')\n",
    "qa_attempt1(llm=chatgpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the response is plausible, ChatGPT did not extract any conversation segments to ground the answer (and thus fails our specification). Let's try a different prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_attempt2 = guidance('''{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Consider the following question:\n",
    "Question: {{query}}\n",
    "Now follow these steps:\n",
    "1. Extract from the transcript the most relevant segments for the answer\n",
    "2. Answer the question.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}''')\n",
    "qa_attempt2(llm=chatgpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, but maybe we want to specify the output format a little more, e.g. let's say we want each segment to have a summary, and to keep the character names. \n",
    "Let's try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_attempt3 = guidance('''{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Based on the above, please answer the following question:\n",
    "Question: {{query}}\n",
    "Please extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\n",
    "Note that conversation segments can be of any length, e.g. including multiple conversation turns.\n",
    "Please extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\n",
    "\n",
    "As an example of output format, here is a fictitious answer to a question about another meeting transcript.\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: Peter and John discuss the weather.\n",
    "Peter: John, how is the weather today?\n",
    "John: It's raining.\n",
    "Segment 2: Peter insults John\n",
    "Peter: John, you are a bad person.\n",
    "Segment 3: Blank\n",
    "ANSWER: Peter and John discussed the weather and Peter insulted John.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}''')\n",
    "qa_attempt3(llm=chatgpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT did extract relevant segments, but it did not follow our output format (it did not summarize each segment, nor did it have the participant's names). Let's try again, with more explicit instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_attempt4 = guidance('''{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Based on the above, please answer the following question:\n",
    "Question: {{query}}\n",
    "Please extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\n",
    "Note that conversation segments can be of any length, e.g. including multiple conversation turns.\n",
    "Please extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\n",
    "\n",
    "Your output should have the following structure:\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: a summary of the first conversation segment\n",
    "(segment here)\n",
    "Segment 2: a summary of the second conversation segment\n",
    "(segment here)\n",
    "Segment 3: a summary of the third conversation segment\n",
    "(segment here)\n",
    "ANSWER: the answer to the question, supported by the segments above.\n",
    "\n",
    "As an example of output format, here is a fictitious answer to a question about another meeting transcript.\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: Peter and John discuss the weather.\n",
    "Peter: John, how is the weather today?\n",
    "John: It's raining.\n",
    "Segment 2: Peter insults John\n",
    "Peter: John, you are a bad person.\n",
    "Segment 3: Blank\n",
    "ANSWER: Peter and John discussed the weather and Peter insulted John.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}''')\n",
    "qa_attempt4(llm=chatgpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we got ChatGPT to use the format we wanted.\n",
    "The root of the problem we're facing is that the OpenAI API does not allow us to do partial output completion (i.e. we can't specify how the assistant begins to answer), and thus it's hard for us to **guide** the output.  \n",
    "If, instead, we use one of the open source models, we can guide the output more clearly, forcing the model to use our structure.  \n",
    "For example, here is how we might modify `qa_attempt3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_guided = guidance('''{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Based on the above, please answer the following question:\n",
    "Question: {{query}}\n",
    "Please extract the three segment from the transcript that are the most relevant for the answer, and then answer the question.\n",
    "Note that conversation segments can be of any length, e.g. including multiple conversation turns. If you need less than three segments, you can leave the rest blank.\n",
    "\n",
    "As an example of output format, here is a fictitious answer to a question about another meeting transcript:\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: Peter and John discuss the weather.\n",
    "Peter: John, how is the weather today?\n",
    "John: It's raining.\n",
    "Segment 2: Peter insults John\n",
    "Peter: John, you are a bad person.\n",
    "Segment 3: Blank\n",
    "ANSWER: Peter and John discussed the weather and Peter insulted John.\n",
    "{{/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: {{gen 'segment1' temperature=0}}\n",
    "Segment 2: {{gen 'segment2' temperature=0}}\n",
    "Segment 3: {{gen 'segment3' temperature=0}}\n",
    "ANSWER: {{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}''')\n",
    "qa_guided(llm=vicuna, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this guidance, we get the right format the first time (and all the time). Let's see how MPT does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_guided(llm=mpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While MPT follows the format, it ignores the question and takes snippets from the format example rather than from the real transcript.  \n",
    "From now on, we'll just compare ChatGPT and Vicuna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = 'Who wants to sell the company?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_attempt4(llm=chatgpt, system_prompt=chatgpt_system, transcript=meeting_transcript, query=query2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we didn't fix the formatting issue with ChatGPT yet. Not only does it not summarize the segments, it also doesn't really answer the question. Let's try again, putting a one-shot example as a conversation round this time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_attempt5 = guidance('''{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: What were the main things that happened in the meeting?\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "Peter: Hey\n",
    "John: Hey\n",
    "Peter: John, how is the weather today?\n",
    "John: It's raining.\n",
    "Peter: That's too bad. I was hoping to go for a walk later.\n",
    "John: Yeah, it's a shame.\n",
    "Peter: John, you are a bad person.\n",
    "----\n",
    "Based on the above, please answer the following question:\n",
    "Question: {{query}}\n",
    "Please extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\n",
    "Note that conversation segments can be of any length, e.g. including multiple conversation turns.\n",
    "Please extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: Peter and John discuss the weather.\n",
    "Peter: John, how is the weather today?\n",
    "John: It's raining.\n",
    "Segment 2: Peter insults John\n",
    "Peter: John, you are a bad person.\n",
    "Segment 3: Blank\n",
    "ANSWER: Peter and John discussed the weather and Peter insulted John.\n",
    "{{~/assistant~}}\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Based on the above, please answer the following question:\n",
    "Question: {{query}}\n",
    "Please extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\n",
    "Note that conversation segments can be of any length, e.g. including multiple conversation turns.\n",
    "Please extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_attempt5(llm=chatgpt, transcript=meeting_transcript, query=query2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works this time (it also works on the original query):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_attempt5(llm=chatgpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how Vicuna does on this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_guided(llm=vicuna, transcript=meeting_transcript, query=query2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna just worked.  \n",
    "Let's now try both of these prompts on a different meeting transcript, the beginning of [an interview](https://www.rev.com/blog/transcripts/elon-musk-interview-with-the-bbc-4-11-23-transcript) with Elon Musk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full transcript: https://www.rev.com/blog/transcripts/elon-musk-interview-with-the-bbc-4-11-23-transcript\n",
    "transcript2 = '''Interviewer: In Sachs that used to be in content moderation. And we’ve spoken to people very recently who were involved in moderation. And they just say there’s not enough people to police this stuff. Particularly around hate speech in the company. Is that something that you-\n",
    "Elon Musk: What hate speech are you talking about? I mean, you use Twitter?\n",
    "Interviewer: Right.\n",
    "Elon Musk: Do you see a rise in hate speech? Just your personal anecdote, do you? I don’t.\n",
    "Interviewer: Personally, my For You, I would see I get more of that kind of content. Yeah. Personally. But I’m not going to talk for the rest of Twitter.\n",
    "Elon Musk: You see more hate speech personally?\n",
    "Interviewer: I would say see more hateful content in that.\n",
    "Elon Musk: Content you don’t like, or hateful. Describe a hateful thing?\n",
    "Interviewer: Yeah, I mean just content that will solicit a reaction. Something that may include something that is slightly racist or slightly sexist. Those kinds of things.\n",
    "Elon Musk: So you think if it’s something is slightly sexist it should be banned?\n",
    "Interviewer: No, I’m not saying anything. I’m saying-\n",
    "Elon Musk: I’m just curious. I’m trying to understand what you mean by “hateful content.” And I’m asking for specific examples. And you just said that if something is slightly sexist, that’s hateful content. Does that mean that it should be banned?\n",
    "Interviewer: Well, you’ve asked me whether my feed, whether it’s got less or more. I’d say it’s got slightly more.\n",
    "Elon Musk: That’s why I’m asking for examples. Can you name one example?\n",
    "Interviewer: I honestly don’t…\n",
    "Elon Musk: You can’t name a single example?\n",
    "Interviewer: I’ll tell you why. Because I don’t actually use that For You feed anymore. Because I just don’t particularly like it. Actually a lot of people are quite similar. I only look at my following.\n",
    "Elon Musk: You said you’ve seen more hateful content, but you can’t name a single example. Not even one.\n",
    "Interviewer: I’m not sure I’ve used that feed for the last three or four weeks. And I honestly couldn’t-\n",
    "Elon Musk: Then how could you see the hateful content?\n",
    "Interviewer: Because I’ve been using it. I’ve been using Twitter since you’ve taken it over for the last six months.\n",
    "Elon Musk: Then you must have at some point seen the For You hateful content. I’m asking for one example.\n",
    "Interviewer: Right.\n",
    "Elon Musk: And you can’t give a single one.\n",
    "Interviewer: And I’m saying-\n",
    "Elon Musk: Then I say, sir, that you don’t know what you’re talking about.\n",
    "Interviewer: Really?\n",
    "Elon Musk: Yes. Because you can’t give a single example of hateful content. Not even one tweet. And yet you claimed that the hateful content was high. That’s false.\n",
    "Interviewer: No. What I claimed-\n",
    "Elon Musk: You just lied.\n",
    "Interviewer: No, no. What I claimed was there are many organizations that say that that kind of information is on the rise. Now whether it has on my feed or not...'''\n",
    "query3 = 'The interviewer says his claim was not about his personal feed. Is this true?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_attempt5(llm=chatgpt, transcript=transcript2, query=query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_guided(llm=vicuna, transcript=transcript2, query=query3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, both work fine. Another question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query4 = 'Does Elon Musk insult the interviewer?'\n",
    "qa_attempt5(llm=chatgpt, transcript=transcript2, query=query4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_guided(llm=vicuna,  transcript=transcript2, query=query4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna, has the right format and even the right segments, but it surprisingly generates a completely wrong answer, when it says \"Elon musk does not accuse him of lying or insult him in any way\".   \n",
    "We tried a variety of other questions and conversations, and the overall pattern was that Vicuna was comparable to ChatGPT on most questions, but got the answer wrong more often than ChatGPT did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Using Bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to get these LLMs to iteratively use a bash shell to solve individual tasks.\n",
    "Whenever they issue a command, we run it and paste the output back into the prompt, until the task is solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bash session with state\n",
    "import pty\n",
    "from subprocess import Popen\n",
    "import os\n",
    "import time\n",
    "class BashSession:\n",
    "    def __init__(self):\n",
    "        self.master_fd, self.slave_fd = pty.openpty()\n",
    "        self.p = Popen('bash',\n",
    "              preexec_fn=os.setsid,\n",
    "              stdin=self.slave_fd,\n",
    "              stdout=self.slave_fd,\n",
    "              stderr=self.slave_fd,\n",
    "              universal_newlines=True)\n",
    "        self.run('ls')\n",
    "    def run(self, command):\n",
    "        command = command + '\\n'\n",
    "        os.write(self.master_fd, command.encode())\n",
    "        time.sleep(0.2)\n",
    "        return '\\n'.join(os.read(self.master_fd, 10240).decode().split('\\n')[1:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do ChatGPT. Again, since we can't specify the output format, we rely on a description of the format and on a one-shot example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "terminal = guidance('''{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "Please complete the following task:\n",
    "Task: list the files in the current directory\n",
    "You can give me one bash command to run at a time, using the syntax:\n",
    "COMMAND: command\n",
    "I will run the commands on my terminal, and paste the output back to you. Once you are done with the task, please type DONE.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "COMMAND: ls\n",
    "{{~/assistant~}}\n",
    "{{#user~}}\n",
    "Output: guidance project\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "The files or folders in the current directory are:\n",
    "- guidance\n",
    "- project\n",
    "DONE\n",
    "{{~/assistant~}}\n",
    "{{#user~}}\n",
    "Please complete the following task:\n",
    "Task: {{task}}\n",
    "You can give me one bash command to run at a time, using the syntax:\n",
    "COMMAND: command\n",
    "I will run the commands on my terminal, and paste the output back to you. Once you are done with the task, please type DONE.\n",
    "{{/user}}\n",
    "{{#geneach 'commands' stop=False}}\n",
    "{{#assistant~}}\n",
    "{{gen 'this.command' temperature=0}}\n",
    "{{~/assistant~}}\n",
    "{{#user~}}\n",
    "Output: {{set 'this.output' (await 'output')}}\n",
    "{{~/user}}\n",
    "{{/geneach}}''')\n",
    "def run_task_chatgpt(task):\n",
    "    t = terminal(llm=chatgpt, task=task)\n",
    "    session = BashSession()\n",
    "    for _ in range(10):\n",
    "        # Extract command\n",
    "        command = re.findall(r'COMMAND: (.*)', t['commands'][-1]['command'])\n",
    "        if not command or 'DONE' in t['commands'][-1]['command']:\n",
    "            break\n",
    "        command = command[0]\n",
    "        output = session.run(command)\n",
    "        t = t(output=output)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_task_chatgpt(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try a simple task.  \n",
    "We created a dummy repo in `~/work/project`, with file `license.txt` (not the standard `LICENSE` file name).  \n",
    "Without communicating this to ChatGPT, let's see if it can figure it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'Find out what license the open source project located in ~/work/project is using.'\n",
    "run_task_chatgpt(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, ChatGPT follows a very natural sequence, and solves the task.\n",
    "\n",
    "For the open source models, we write a simpler (guided) prompt where there is a sequence of command-output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_terminal = guidance('''{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "Please complete the following task:\n",
    "Task: list the files in the current directory\n",
    "You can run bash commands using the syntax:\n",
    "COMMAND: command\n",
    "OUTPUT: output\n",
    "Once you are done with the task, use the COMMAND: DONE.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "COMMAND: ls\n",
    "OUTPUT: guidance project\n",
    "COMMAND: DONE \n",
    "{{~/assistant~}}\n",
    "{{#user~}}\n",
    "Please complete the following task:\n",
    "Task: {{task}}\n",
    "You can run bash commands using the syntax:\n",
    "COMMAND: command\n",
    "OUTPUT: output\n",
    "Once you are done with the task, use the COMMAND: DONE.\n",
    "{{~/user}}\n",
    "{{~#assistant~}}\n",
    "{{#geneach 'commands'~}}\n",
    "COMMAND: {{gen 'this.command' stop='\\\\n'}}\n",
    "OUTPUT: {{shell this.command}}{{~/geneach}}\n",
    "{{~/assistant~}}''')\n",
    "class StatefulShellOpenSource:\n",
    "    def __init__(self):\n",
    "        self.session = BashSession()\n",
    "    def __call__(self, command):\n",
    "        if 'DONE' in command:\n",
    "            raise StopIteration\n",
    "        output = self.session.run(command)\n",
    "        return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell = StatefulShellOpenSource()\n",
    "task = 'Find out what license the open source project located in ~/work/project is using.'\n",
    "t = guided_terminal(llm=vicuna, task=task, shell=shell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna is not able to solve the task this time. Let's see how MPT does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell = StatefulShellOpenSource()\n",
    "t = guided_terminal(llm=mpt, task=task, shell=shell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, MPT works this time while Vicuna doesn't.\n",
    "\n",
    "Besides privacy (we're not sending the session transcript to OpenAI), open source-models have a significant advantage in Guidance right now: the whole prompt is a single LLM run (and we even [accelerate](https://github.com/microsoft/guidance#guidance-acceleration-notebook) it by not having it geneate the output structure tokens like `COMMAND:`).   \n",
    "\n",
    "In contrast, we have to make a new call to ChatGPT for each command, which is slower and more expensive.\n",
    "\n",
    "Let's try a different bash task with ChatGPT and Vicuna:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'Find all jupyter notebook files in ~/work/guidance that are currently untracked by git'\n",
    "run_task_chatgpt(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we run into a problem with ChatGPT not following our specified output structure (and thus making it impossible for us to use inside a program, without a human in the loop).  \n",
    "We fix this __particular__ problem by changing the message when there is no output below, but we can't fix the general problem of not being able to _force_ ChatGPT to follow our specified output structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    t = terminal(llm=chatgpt, task=task)\n",
    "    session = BashSession()\n",
    "    for _ in range(10):\n",
    "        # Extract command\n",
    "        command = re.findall(r'COMMAND: (.*)', t['commands'][-1]['command'])\n",
    "        if not command or 'DONE' in t['commands'][-1]['command']:\n",
    "            break\n",
    "        command = command[0]\n",
    "        output = session.run(command)\n",
    "        if not output:\n",
    "            output = 'No output'\n",
    "        t = t(output=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ChatGPT _was_ able to solve the problem after this small modification. Let's see how Vicuna does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell = StatefulShellOpenSource()\n",
    "t = guided_terminal(llm=vicuna, task=task, shell=shell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna follows our output structure, but unfortunately runs the wrong command to do the task.  \n",
    "We observed the same pattern for other tasks as well, and found ChatGPT to be more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell = StatefulShellOpenSource()\n",
    "t = guided_terminal(llm=mpt, task=task, shell=shell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, MPT fails this time, by calling the same command again and again (we had to interrupt to stop execution.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a discussion of these experiments check out the [the blog post](https://medium.com/@marcotcr/exploring-chatgpt-vs-open-source-models-on-slightly-harder-tasks-aa0395c31610)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
